# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xu4OV6y4jYTyHFk1gS-jbsaSFoRFLkxu
"""

import requests
import zipfile
import io
import os
import json
import time
GITHUB_TOKEN = ""
HEADERS = {"Authorization": f"token {GITHUB_TOKEN}"} if GITHUB_TOKEN else {}

SEARCH_QUERIES = [
    # Arduino General
    "arduino projects",
    "arduino libraries",
    "arduino examples",
    "arduino robotics",

    # Arduino Sensors & Actuators
    "arduino sensor library",
    "arduino temperature humidity sensor",
    "arduino ultrasonic sensor",
    "arduino servo motor",
    "arduino stepper motor",
    "arduino relay control",
    "arduino i2c spi",
    "arduino oled lcd display",
    "arduino dht11 dht22",
    "arduino bmp180 bmp280",

    # Raspberry Pi General
    "raspberry pi projects",
    "raspberry pi python examples",
    "raspberry pi gpio python",
    "raspberry pi robotics",

    # Raspberry Pi Sensors & Actuators
    "raspberry pi sensor python",
    "raspberry pi camera python",
    "raspberry pi ultrasonic sensor",
    "raspberry pi gyroscope accelerometer",
    "raspberry pi i2c spi python",
    "raspberry pi temperature humidity sensor",

    # ESP32 / MicroPython
    "esp32 projects",
    "esp32 micropython examples",
    "esp32 sensor micropython",
    "esp32 iot micropython",
    "esp32 wifi bluetooth",

    # CircuitPython
    "circuitpython sensor driver",
    "circuitpython display",
    "circuitpython robotics",
]

REPOS_PER_QUERY = 10
# File extensions to include in the final dataset
TARGET_EXTENSIONS = ['.py', '.ino', '.cpp', '.h', '.c']
# Output directory and file names
DOWNLOADS_DIR = "repo_downloads"
DATASET_DIR = "dataset"
DATASET_FILE = os.path.join(DATASET_DIR, "llm_finetuning_dataset.txt")
GITHUB_API_BASE = "https://api.github.com"

def search_github_repos(query, per_page):
    """
    Searches GitHub for repositories matching a query.
    """
    print(f"\nSearching for query: '{query}'...")
    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json"
    }
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc",
        "per_page": per_page
    }
    search_url = f"{GITHUB_API_BASE}/search/repositories"

    try:
        response = requests.get(search_url, headers=headers, params=params)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Handle rate limiting
        if response.status_code == 403:
            print("Rate limit likely exceeded. Waiting before retrying...")
            time.sleep(60)
            response = requests.get(search_url, headers=headers, params=params)
            response.raise_for_status()

        data = response.json()
        repo_names = [item['full_name'] for item in data.get('items', [])]
        print(f"Found {len(repo_names)} repositories.")
        return repo_names

    except requests.exceptions.RequestException as e:
        print(f"An error occurred during API request: {e}")
        return []

def download_and_unzip_repo(repo_full_name):
    """
    Downloads a repository as a ZIP file and extracts it.
    """
    owner, repo = repo_full_name.split('/')
    print(f"Downloading and extracting '{repo_full_name}'...")

    # The API endpoint provides a redirect to the zipball, which requests handles automatically
    zip_url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/zipball"
    headers = {"Authorization": f"token {GITHUB_TOKEN}"}

    try:
        response = requests.get(zip_url, headers=headers, stream=True, timeout=60)
        response.raise_for_status()

        # Extract to a specific folder to avoid name collisions
        extract_path = os.path.join(DOWNLOADS_DIR, repo_full_name.replace('/', '_'))

        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(extract_path)
        print(f"Successfully extracted to '{extract_path}'")
        return True

    except requests.exceptions.RequestException as e:
        print(f"Failed to download {repo_full_name}. Error: {e}")
        return False
    except zipfile.BadZipFile:
        print(f"Failed to unzip {repo_full_name}. The file might be corrupted or not a zip file.")
        return False

def create_dataset():
    """
    Walks through the downloaded repositories and compiles source code into a single dataset file.
    """
    print(f"\nCreating dataset from source files in '{DOWNLOADS_DIR}'...")
    file_count = 0

    # Ensure the dataset directory exists
    os.makedirs(DATASET_DIR, exist_ok=True)

    with open(DATASET_FILE, 'w', encoding='utf-8', errors='ignore') as outfile:
        if not os.path.exists(DOWNLOADS_DIR):
            print(f"Error: Download directory '{DOWNLOADS_DIR}' not found.")
            return

        for root, _, files in os.walk(DOWNLOADS_DIR):
            for file in files:
                if any(file.endswith(ext) for ext in TARGET_EXTENSIONS):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                            content = infile.read()
                            # Write a separator and file path for context before the content
                            outfile.write(f"# --- START FILE: {os.path.relpath(file_path, DOWNLOADS_DIR)} ---\n\n")
                            outfile.write(content)
                            outfile.write(f"\n\n# --- END FILE: {os.path.relpath(file_path, DOWNLOADS_DIR)} ---\n\n")
                            file_count += 1
                    except Exception as e:
                        print(f"Could not read file {file_path}: {e}")

    print(f"Dataset creation complete. Combined {file_count} files into '{DATASET_FILE}'.")

def main():
    """
    Main function to orchestrate the entire process.
    """
    print("--- Starting GitHub Repository Scraper and Dataset Creator ---")

    # Create the main download directory
    os.makedirs(DOWNLOADS_DIR, exist_ok=True)

    all_repos = set()
    for query in SEARCH_QUERIES:
        repos = search_github_repos(query, REPOS_PER_QUERY)
        all_repos.update(repos)
        # Be respectful to the API rate limits
        time.sleep(5)

    print(f"\nTotal unique repositories to download: {len(all_repos)}")

    downloaded_count = 0
    for repo_name in all_repos:
        if download_and_unzip_repo(repo_name):
            downloaded_count += 1
        # Add a small delay between downloads
        time.sleep(1)

    if downloaded_count > 0:
        create_dataset()
    else:
        print("\nNo repositories were downloaded. Skipping dataset creation.")

    print("\n--- Process Finished ---")


if __name__ == "__main__":
    main()

!du -h /content/dataset/llm_finetuning_dataset.txt